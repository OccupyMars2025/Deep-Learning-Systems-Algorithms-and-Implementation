# -*- coding: utf-8 -*-
"""Copy of 8_nn_library_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t5fzNiZhdhycdc0farI8n8izS1wrA39P

# Lecture 8: Neural Network Library Implementation

In this lecture, we will to walk through neural network library design.

## Prepare the codebase

To get started, we can clone the related lecture8 repo from the github.
"""
import sys
# append the "needle" path, so you can import needle
sys.path.append("./python")
import needle as ndl

import numpy as np

class Parameter(ndl.Tensor):
    """parameter"""

def _get_params(value):
    if isinstance(value, Parameter):
        return [value]
    if isinstance(value, dict):
        params = []
        for k, v in value.items():
            params += _get_params(v)
        return params
    if isinstance(value, Module):
        return value.parameters()
    return []

class Module:
    def parameters(self):
        return _get_params(self.__dict__)

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)

"""Now that we have the base Module interface, we can start to define different kind of modules. Let us define a simple scale add module, that computes $y = x \times s + b$. The ScaleAdd is parameterized by $s$ and $b$."""

class ScaleAdd(Module):
    def __init__(self, init_s=1, init_b=0):
        self.s = Parameter([init_s], dtype="float32")
        self.b = Parameter([init_b], dtype="float32")
    
    def forward(self, x):
        return x * self.s + self.b

"""We allow a module to contain multiple submodules inside and compose them together"""

class MultiPathScaleAdd(Module):
    def __init__(self):
        self.path0 = ScaleAdd()
        self.path1 = ScaleAdd()

    def forward(self, x):
        return self.path0(x) + self.path1(x)

mpath = MultiPathScaleAdd()
mpath.parameters()

"""### Loss function

**NOTE**: You will need to fill in your implementation of autograd in hw1, restart the runtime in order to run the following piece of code.
"""

class L2Loss(Module):
    def forward(self, x ,y):
        z = x + (-1) * y
        return z * z

# x = ndl.Tensor([3], dtype="float32")
# y = ndl.Tensor([2], dtype="float32")
# loss = L2Loss()(mpath(x), y)

# loss

# loss.backward()

# x.grad, y.grad

# params = mpath.parameters()

# params

"""### Optimizer
We are now ready to define the optimizer interface. There are two key functions here:

- reset_grad: reset the gradient fields of each the the parameters
- step: update the parameters
"""



class Optimizer:
    def __init__(self, params):
        self.params = params

    def reset_grad(self):
        for p in self.params:
            p.grad = None
        
    def step(self):
        raise NotImplemented()

class SGD(Optimizer):
    def __init__(self, params, lr):
        self.params = params
        self.lr = lr

    def step(self):
        for w in self.params:
            # Caution: These two w.data have different meaning
            w.data = w.data + (-self.lr) * w.grad
            # The following implementation is wrong!!!
            # w = w.data + (-self.lr) * w.grad

x = ndl.Tensor([3], dtype="float32")
y = ndl.Tensor([2], dtype="float32")

model = MultiPathScaleAdd()
l2loss = L2Loss()
opt = SGD(model.parameters(), lr=0.01)
num_epoch = 10

for epoch in range(num_epoch):
    opt.reset_grad()
    h = model(x)
    loss = l2loss(h, y)
    training_loss = loss.numpy()
    print(training_loss)

    loss.backward()
    opt.step()


"""### Initialization

In the homework, you will need to implement the intialization function for the weight in linear transformations. Under a linear relu network where $y^{(l)} = x^{(l-1)} W^T, x^{(l)} = max(y^{(l)}, 0)$. Assume that $W\in R^{n_{out} \times n_{in}}$ A common way to do so is to intialize it as $\mathcal{N}(0, \sigma^2)$ where $\sigma = \sqrt{\frac{2}{n_{in}}}$.

Checkout Explaination from the original paper: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

\begin{equation}
 y_i = \sum_{j=1}^{n_{in}} x_j W_{i, j}   
\end{equation}


\begin{equation}
 \mbox{Var}[y_i] =  n_{in} E[x_0^2] \mbox{Var}[W_{i, j}] = n_{in} E[x_0^2] \sigma^2
\end{equation}

Considering the fact that x is also a result of relu of previous layer 

\begin{equation}
 E[x_0^2] = E[relu(y^{(l-1)})^2] = \frac{1}{2}  \mbox{Var}[y^{(l-1)}]
\end{equation}

We can get the variance value by requiring $\mbox{Var}[y^{(l)}] = \mbox{Var}[y^{(l-1)}]$.
NOTE: the variance value was derived under a specific deep relu network.

## Additional contents on programming model

In this section, we will review additional contents on autograd that we may need in future lectures.

### Fused operator and Tuple Value

Up until now each of the needle operator only returns a single output Tensor. In real world application scenarios, it is somtimes helpful to compute many outputs at once in a single (fused) operator.

Needle is designed to support this feature. In order to do so, we need to introduce a new kind of Value -- Tuple.

Open up the files on the left side panel, review the the following changes 
- `autograd.py`: The TensorTuple class
- `ops.py`: TupleGetItemOp and MakeTupleOp
"""
print(20*"=" + "Tuple")
x = ndl.Tensor([1], dtype="float32")

z = ndl.ops.fused_add_scalars(x, 1, 2)

print(z)

v0 = z[0]

v0.op